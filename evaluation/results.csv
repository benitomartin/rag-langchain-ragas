question,answer,contexts,ground_truth,faithfulness,answer_relevancy,context_recall,context_precision,answer_correctness
What types of examples are included in HaluEval and what tasks do they cover?,"HaluEval covers question answering, dialogue, and summarization tasks. The dataset includes 5k general user queries with ChatGPT responses and 30k task-specific examples from these three tasks.","[""HaluEval includes 5k general user queries with ChatGPT responses and 30k task-specific examples from three tasks: question answering, (knowledge-grounded) dialogue, and summarisation – which we refer to as HaluEval QA/Dialogue/Summarisation, respectively. In HaluEval QA, the model is given a question (e.g., “Which magazine was started first Arthur's Magazine or First for Women?”), a knowledge snippet (e.g., “Arthur's Magazine (1844–1846) was an American literary periodical published in Philadelphia in the""
 'We consider two hallucination detection tasks, namely SelfCheckGPT — which checks if a model produces self-consistent answers — and HaluEval, which checks whether a model can identify faithfulness hallucinations in QA, Dialog, and Summarisation tasks with respect to a given snippet of knowledge.'
 'Hallucination Detection. FaithDial, True-False, and HaluEval QA/Dialogue/Summarisation are designed to target hallucination detection in LLMs specifically.']","HaluEval includes examples from three tasks: question answering, dialogue, and summarization.",1.0,0.9405822527035852,1.0,0.9999999999666667,0.612962376566289
How is the model's ability to accurately and faithfully perform tasks evaluated in IFEval?,"IFEval evaluates the model's ability to accurately and faithfully perform tasks by presenting it with a set of instructions to execute in a zero-shot setting. The model's performance is assessed through various metrics such as ROUGE, factKB, and BERTScore-Precision. These metrics measure the model's ability to generate text that is similar to the reference text, while also ensuring that the generated text is factually accurate and follows the given instructions.","['and the model needs to follow the instruction in the prompt in a zero-shot setting. IFEval (Instruction Following Evaluation) presents the model with a set of instructions to execute, evaluating its ability to accurately and faithfully perform tasks as instructed. An IFEval instance is composed by a prompt (e.g., Write a 300+ word summary of the wikipedia page [..]. Do not use any commas and highlight at least 3 sections that have titles in markdown format, for example [..]”), and the model is evaluated on'
 'Perhaps surprisingly, one of the best models on MemoTrap is BLOOM 560M and, in general, smaller models tend to have strong results on this dataset. As the Inverse Scaling Prize evidenced, larger models tend to memorize famous quotes and therefore score poorly in this task. Instructions in IFEval tend to be significantly harder to follow (as each instance involves complying with several constraints on the generated text) – the best results so far tend to be produced by LLaMA2 13B Chat and Mistral 7B'
 'which is critical in maintaining the integrity of news reporting. For assessing the faithfulness of the model to the original document, we use several metrics: ROUGE, which measures the overlap between the generated text and the reference text; factKB, a model-based metric for factuality evaluation that is generalisable across domains; and BERTScore-Precision, a metric based on BERTScore, which computes the similarity between two texts by using the similarities between their token representations. For both']",The model's ability to accurately and faithfully perform tasks in IFEval is evaluated by presenting it with a set of instructions to execute and assessing its performance in following those instructions.,1.0,0.9346766999866519,1.0,0.9999999999,0.6561402997720647
"Which models have achieved the best results in IFEval and what are their respective sizes, considering that smaller models tend to perform well on this dataset and larger models tend to struggle due to memorizing famous quotes?","The best models that have achieved high scores in both MemoTrap and IFEval are:
* BLOOM 560M: This model has shown excellent performance in both datasets, particularly in the IFEval evaluation, where it scored 94.8%.
* LLaMA2 13B Chat: This model also performed well in both datasets, with a score of 90.6% in IFEval.
* Mistral 7B: This model showed good performance in MemoTrap, with a score of 86.4%, and decent performance in IFEval, with a score of 83.3%.","['Perhaps surprisingly, one of the best models on MemoTrap is BLOOM 560M and, in general, smaller models tend to have strong results on this dataset. As the Inverse Scaling Prize evidenced, larger models tend to memorize famous quotes and therefore score poorly in this task. Instructions in IFEval tend to be significantly harder to follow (as each instance involves complying with several constraints on the generated text) – the best results so far tend to be produced by LLaMA2 13B Chat and Mistral 7B'
 'Instruction Following. MemoTrap and IFEval are designed to test how well a model follows specific instructions. MemoTrap (we use the version used in the Inverse Scaling Prize) is a dataset spanning text completion, translation, and QA, where repeating memorised text and concept is not the desired behaviour. An example in MemoTrap is composed by a prompt (e.g., “Write a quote that ends in the word ""heavy"": Absence makes the heart grow”) and two possible completions (e.g., “heavy” and “fonder”), and the'
 'and the model needs to follow the instruction in the prompt in a zero-shot setting. IFEval (Instruction Following Evaluation) presents the model with a set of instructions to execute, evaluating its ability to accurately and faithfully perform tasks as instructed. An IFEval instance is composed by a prompt (e.g., Write a 300+ word summary of the wikipedia page [..]. Do not use any commas and highlight at least 3 sections that have titles in markdown format, for example [..]”), and the model is evaluated on']",The models that have achieved the best results in IFEval are LLaMA2 13B Chat and Mistral 7B. The respective sizes of these models are not mentioned in the context.,0.0,0.8860742450606981,0.5,0.0,0.5253563180626293
"""What datasets evaluate models' summarisation capabilities and what types of summaries do they include, based on the context and Hallucinations Leaderboard?""","There are several datasets that evaluate models' summarization capabilities, including the Hallucinations Leaderboard, which assesses LLMs on various hallucination-related benchmarks. These datasets typically involve generating summaries from text prompts, often with the goal of producing accurate and informative summaries. Some examples of datasets used to evaluate models' summarization capabilities include:
* The Stanford Summarization Dataset, which contains news articles and their corresponding summaries.
* The CNN/Daily Mail News Summary Dataset, which consists of news articles from CNN and Daily Mail, along with their corresponding summaries.
* The Gigaword Dataset, which includes news articles from various sources, as well as their corresponding summaries.
In addition to these datasets, there are also benchmarks such as the GLUE Benchmark and the SuperGLUE Benchmark, which evaluate models' performance on a variety of natural language processing tasks, including summarization.","['The Hallucinations Leaderboard is an open effort to address the challenge of hallucinations in LLMs. Hallucinations in LLMs, whether in the form of factuality or faithfulness errors, can significantly impact the reliability and usefulness of LLMs in real-world settings. By evaluating a diverse range of LLMs across multiple benchmarks, the Hallucinations Leaderboard aims to provide insights into the generalisation properties and limitations of these models and their tendency to generate hallucinated'
 'The Hallucinations Leaderboard evaluates LLMs on an array of hallucination-related benchmarks. The leaderboard leverages the EleutherAI Language Model Evaluation Harness, a framework for zero-shot and few-shot language model evaluation (via in-context learning) on a wide array of tasks. The Harness is under very active development: we strive to always use the latest available version in our experiments, and keep our results up-to-date. The code (backend and front-end) is a fork of the Hugging Face'
 'This initiative wants to aid researchers and engineers in identifying the most reliable models, and potentially drive the development of LLMs towards more accurate and faithful language generation. The Hallucinations Leaderboard is an evolving project, and we welcome contributions (fixes, new datasets and metrics, computational resources, ideas, ...) and feedback: if you would like \nto work with us on this project, remember to reach out!']","The datasets that evaluate models' summarisation capabilities are XSum and CNN/DM. XSum includes professionally written single-sentence summaries of BBC news articles, while CNN/DM consists of news articles paired with multi-sentence summaries.",0.0,0.9049969630456189,1.0,0.3333333333,0.42960186007332773
