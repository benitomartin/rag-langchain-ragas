question,contexts,ground_truth,evolution_type,episode_done
What is the purpose of the Stanford Question Answering Dataset v2 and how does it test a model's ability to avoid hallucinations in scenarios with insufficient or ambiguous information?,"['to do before she fell over?”), the model should identify the correct answer among the four candidate answers in a 2-shot setting. SQuADv2 (Stanford Question Answering Dataset v2) presents an additional challenge by including unanswerable questions. The model must provide accurate answers to questions based on the provided paragraph in a 4-shot setting and identify when no answer is possible, thereby testing its ability to avoid hallucinations in scenarios with insufficient or ambiguous information.']","The purpose of the Stanford Question Answering Dataset v2 is to test a model's ability to provide accurate answers to questions based on a provided paragraph. It also tests the model's ability to identify when no answer is possible, thereby testing its ability to avoid hallucinations in scenarios with insufficient or ambiguous information.",simple,True
What are some of the best models for generating text on MemoTrap and how do they compare to larger models in terms of performance?,"['Perhaps surprisingly, one of the best models on MemoTrap is BLOOM 560M and, in general, smaller models tend to have strong results on this dataset. As the Inverse Scaling Prize evidenced, larger models tend to memorize famous quotes and therefore score poorly in this task. Instructions in IFEval tend to be significantly harder to follow (as each instance involves complying with several constraints on the generated text) – the best results so far tend to be produced by LLaMA2 13B Chat and Mistral 7B']","Some of the best models for generating text on MemoTrap are BLOOM 560M, LLaMA2 13B Chat, and Mistral 7B. Smaller models tend to have strong results on this dataset, while larger models tend to score poorly as they tend to memorize famous quotes.",simple,True
What is the purpose of the Hallucinations Leaderboard in relation to minimizing hallucinations in text generation?,"['Our comprehensive evaluation process gives a concise ranking of LLMs, allowing users to understand the performance of various models in a more comparative, quantitative, and nuanced manner. We believe that the Hallucinations Leaderboard is an important and ever more relevant step towards making LLMs more reliable and efficient, encouraging the development of models that can better understand and replicate human-like text generation while minimizing the occurrence of hallucinations.']",The purpose of the Hallucinations Leaderboard is to minimize the occurrence of hallucinations in text generation.,simple,True
What types of examples are included in HaluEval and what tasks do they cover?,"[""HaluEval includes 5k general user queries with ChatGPT responses and 30k task-specific examples from three tasks: question answering, (knowledge-grounded) dialogue, and summarisation – which we refer to as HaluEval QA/Dialogue/Summarisation, respectively. In HaluEval QA, the model is given a question (e.g., “Which magazine was started first Arthur's Magazine or First for Women?”), a knowledge snippet (e.g., “Arthur's Magazine (1844–1846) was an American literary periodical published in Philadelphia in the""]","HaluEval includes examples from three tasks: question answering, dialogue, and summarization.",simple,True
How is the model's ability to accurately and faithfully perform tasks evaluated in IFEval?,"['and the model needs to follow the instruction in the prompt in a zero-shot setting. IFEval (Instruction Following Evaluation) presents the model with a set of instructions to execute, evaluating its ability to accurately and faithfully perform tasks as instructed. An IFEval instance is composed by a prompt (e.g., Write a 300+ word summary of the wikipedia page [..]. Do not use any commas and highlight at least 3 sections that have titles in markdown format, for example [..]”), and the model is evaluated on']",The model's ability to accurately and faithfully perform tasks in IFEval is evaluated by presenting it with a set of instructions to execute and assessing its performance in following those instructions.,simple,True
"Which models have achieved the best results in IFEval and what are their respective sizes, considering that smaller models tend to perform well on this dataset and larger models tend to struggle due to memorizing famous quotes?","['Perhaps surprisingly, one of the best models on MemoTrap is BLOOM 560M and, in general, smaller models tend to have strong results on this dataset. As the Inverse Scaling Prize evidenced, larger models tend to memorize famous quotes and therefore score poorly in this task. Instructions in IFEval tend to be significantly harder to follow (as each instance involves complying with several constraints on the generated text) – the best results so far tend to be produced by LLaMA2 13B Chat and Mistral 7B']",The models that have achieved the best results in IFEval are LLaMA2 13B Chat and Mistral 7B. The respective sizes of these models are not mentioned in the context.,reasoning,True
"""What datasets evaluate models' summarisation capabilities and what types of summaries do they include, based on the context and Hallucinations Leaderboard?""","[""Summarisation. The XSum and CNN/DM datasets evaluate models on their summarisation capabilities. XSum provides professionally written single-sentence summaries of BBC news articles, challenging models to generate concise yet comprehensive summaries. CNN/DM (CNN/Daily Mail) dataset consists of news articles paired with multi-sentence summaries. The model's task is to generate a summary that accurately reflects the article's content while avoiding introducing incorrect or irrelevant information, which is""]","The datasets that evaluate models' summarisation capabilities are XSum and CNN/DM. XSum includes professionally written single-sentence summaries of BBC news articles, while CNN/DM consists of news articles paired with multi-sentence summaries.",multi_context,True
"""What makes MemoTrap and IFEval effective in evaluating a model's ability to comply with complex instructions, and which models have been successful in these evaluations?""","['issue on NQ: LLaMA2 13B is now the best model on this task, with 0.34 EM.']",,multi_context,True
"""What inaccuracies can occur in generated content on the Hallucinations Leaderboard and how do they relate to the concept of hallucinations in large language models?""","['The Hallucinations Leaderboard is an open and ongoing project: if you have any ideas, comments, or feedback, or if you would like to contribute to this project (e.g., by modifying the current tasks, proposing new tasks, or providing computational resources) please reach out!\n\n\n\n\n\n\t\tWhat are Hallucinations?']",,multi_context,True
